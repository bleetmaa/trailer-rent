name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME_BACKEND: ${{ github.repository }}-backend
  IMAGE_NAME_FRONTEND: ${{ github.repository }}-frontend
  KUBERNETES_HOST: ${{ secrets.KUBERNETES_PUBLIC_IP }}

# NOTE: This workflow supports private Kubernetes clusters using insecure-skip-tls-verify
# Ensure your KUBE_CONFIG secret contains a valid kubeconfig for your cluster

jobs:
  test-backend:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '8.0.x'
        
    - name: Restore dependencies
      run: dotnet restore src/backend/TrailerRent.sln
      
    - name: Build
      run: dotnet build src/backend/TrailerRent.sln --no-restore
      
    - name: Test
      run: dotnet test src/backend/TrailerRent.sln --no-build --verbosity normal

  test-frontend:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: src/frontend/trailer-rent-app/package-lock.json
        
    - name: Install dependencies
      run: cd src/frontend/trailer-rent-app && npm ci
      
    - name: Build
      run: cd src/frontend/trailer-rent-app && npm run build
      
    - name: Test
      run: cd src/frontend/trailer-rent-app && npm run test -- --watch=false --browsers=ChromeHeadless

  build-and-push-backend:
    needs: test-backend
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_BACKEND }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: src/backend
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  build-and-push-frontend:
    needs: test-frontend
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME_FRONTEND }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: src/frontend/trailer-rent-app
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy-dev:
    needs: [build-and-push-backend, build-and-push-frontend]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: dev
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
      
    - name: Configure kubectl
      run: |
        mkdir -p $HOME/.kube
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $HOME/.kube/config
        chmod 600 $HOME/.kube/config
        # Set insecure flag globally for this cluster
        kubectl config set-cluster $(kubectl config current-context) --insecure-skip-tls-verify=true
        
    - name: Test cluster connection
      run: |
        # Verify cluster connection (skip TLS verification for self-signed certs)
        kubectl cluster-info --insecure-skip-tls-verify
        kubectl get nodes --insecure-skip-tls-verify
        
    - name: Setup cluster infrastructure
      run: |
        # Install NGINX Ingress Controller if not exists
        if ! kubectl get namespace ingress-nginx --insecure-skip-tls-verify >/dev/null 2>&1; then
          echo "Installing NGINX Ingress Controller..."
          kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml --insecure-skip-tls-verify
          kubectl wait --namespace ingress-nginx \
            --for=condition=ready pod \
            --selector=app.kubernetes.io/component=controller \
            --timeout=300s --insecure-skip-tls-verify
        fi
        
        # Create namespace
        kubectl create namespace trailer-rent-dev --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
        
        # Create secrets
        kubectl create secret generic postgres-secret \
          --from-literal=password="${{ secrets.POSTGRES_PASSWORD }}" \
          --namespace=trailer-rent-dev \
          --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
          
        kubectl create secret generic jwt-secret \
          --from-literal=key="${{ secrets.JWT_SECRET_KEY }}" \
          --namespace=trailer-rent-dev \
          --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
        
    - name: Deploy to Dev
      run: |
        # Update image tags in kustomization
        cd k8s/environments/dev
        kustomize edit set image frontend-image=ghcr.io/${{ github.repository }}-frontend:${{ github.sha }}
        kustomize edit set image backend-image=ghcr.io/${{ github.repository }}-backend:${{ github.sha }}
        cd ../../..
        
        # Apply manifests
        kubectl apply -k k8s/environments/dev --insecure-skip-tls-verify
        kubectl rollout restart deployment/backend deployment/frontend -n trailer-rent-dev --insecure-skip-tls-verify

  deploy-prod:
    needs: [build-and-push-backend, build-and-push-frontend]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
      
    - name: Configure kubectl
      run: |
        mkdir -p $HOME/.kube
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $HOME/.kube/config
        chmod 600 $HOME/.kube/config
        # Set insecure flag globally for this cluster
        kubectl config set-cluster $(kubectl config current-context) --insecure-skip-tls-verify=true
        
    - name: Test cluster connection
      run: |
        # Verify cluster connection (skip TLS verification for self-signed certs)
        kubectl cluster-info --insecure-skip-tls-verify
        kubectl get nodes --insecure-skip-tls-verify
        
    - name: Setup cluster infrastructure
      run: |
        # Create storage directory on worker node for PostgreSQL
        echo "Setting up local storage for PostgreSQL..."
        kubectl apply -f k8s/setup-storage-job.yaml --insecure-skip-tls-verify
        kubectl wait --for=condition=complete job/setup-storage-job --timeout=300s --insecure-skip-tls-verify
        
        # Delete PostgreSQL pod and PVC if they exist (to allow PV recreation)
        kubectl delete pod -l app=postgres -n trailer-rent-prod --insecure-skip-tls-verify || true
        kubectl delete pvc postgres-pvc -n trailer-rent-prod --insecure-skip-tls-verify || true
        
        # Delete old PersistentVolume if it exists with wrong path
        kubectl delete pv postgres-pv --insecure-skip-tls-verify || true
        
        # Apply storage configuration
        kubectl apply -f k8s/storage-setup.yaml --insecure-skip-tls-verify
        
        # Verify storage setup
        kubectl get storageclass --insecure-skip-tls-verify
        kubectl get pv --insecure-skip-tls-verify
        
        # Create namespace
        kubectl create namespace trailer-rent-prod --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
        
        # Create secrets
        kubectl create secret generic postgres-secret \
          --from-literal=password="${{ secrets.POSTGRES_PASSWORD }}" \
          --namespace=trailer-rent-prod \
          --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
          
        kubectl create secret generic jwt-secret \
          --from-literal=key="${{ secrets.JWT_SECRET_KEY }}" \
          --namespace=trailer-rent-prod \
          --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify
        
        # Clean up storage setup job
        kubectl delete job setup-storage-job --insecure-skip-tls-verify || true
        
    - name: Deploy to Production
      run: |
        # Update image tags in kustomization
        cd k8s/environments/prod
        kustomize edit set image frontend-image=ghcr.io/${{ github.repository }}-frontend:${{ github.sha }}
        kustomize edit set image backend-image=ghcr.io/${{ github.repository }}-backend:${{ github.sha }}
        cd ../../..
        
        # Apply manifests
        kubectl apply -k k8s/environments/prod --insecure-skip-tls-verify
        
        # Clean up old pods to ensure fresh deployment
        kubectl delete pods -l app=backend -n trailer-rent-prod --insecure-skip-tls-verify || true
        kubectl delete pods -l app=frontend -n trailer-rent-prod --insecure-skip-tls-verify || true
        
        # Wait for new pods to be ready
        kubectl wait --for=condition=available deployment/backend --timeout=300s -n trailer-rent-prod --insecure-skip-tls-verify || true
        kubectl wait --for=condition=available deployment/frontend --timeout=300s -n trailer-rent-prod --insecure-skip-tls-verify || true
